{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import sys\n",
    "from util.constants import  DATA_DIR, TASKS, NERS\n",
    "from util.preprocessing import readCoNLL, get_label_name\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def build_vocab(domains):\n",
    "    word_count = defaultdict(int)\n",
    "    for domain in domains:\n",
    "        sentences_in_domain = readCoNLL(os.path.join(DATA_DIR, domain, \"train.txt.ori\"), {0: 'tokens', 1: 'labels'})\n",
    "        print(\" Number of sentences in {} : {}\".format(domain, len(sentences_in_domain)))\n",
    "        for sentence_idx in range(len(sentences_in_domain)):\n",
    "            #if sentence_idx % 1000 == 0:\n",
    "            #    print(\"{}\".format(sentence_idx))\n",
    "            tokens = sentences_in_domain[sentence_idx]['tokens']\n",
    "            labels = sentences_in_domain[sentence_idx]['labels']\n",
    "            for token_idx in range(len(tokens)):\n",
    "                token = tokens[token_idx].lower()\n",
    "                word_count[token] += 1\n",
    "    return word_count\n",
    "\n",
    "def build_indexes_from_domains(domains, word_count, threshold = 5):\n",
    "    word2idx = {}\n",
    "    label2idx = {}\n",
    "    domain2label = {}\n",
    "    for domain in domains:\n",
    "        sentences_in_domain = readCoNLL(os.path.join(DATA_DIR, domain, \"train.txt.ori\"), {0: 'tokens', 1: 'labels'})\n",
    "        print(\" Number of sentences in {} : {}\".format(domain, len(sentences_in_domain)))\n",
    "        labels_in_domain = set()\n",
    "        for sentence_idx in range(len(sentences_in_domain)):\n",
    "            tokens = sentences_in_domain[sentence_idx]['tokens']\n",
    "            labels = sentences_in_domain[sentence_idx]['labels']            \n",
    "            for token_idx in range(len(tokens)):\n",
    "                token = tokens[token_idx].lower()\n",
    "                label = get_label_name(labels[token_idx])\n",
    "                if label not in label2idx.keys() and label != \"O\":\n",
    "                    label2idx[label] = len(label2idx)\n",
    "                    labels_in_domain.add(label)\n",
    "                if token not in word2idx.keys() and word_count[token] >= threshold:\n",
    "                    word2idx[token] = len(word2idx)\n",
    "        domain2label[domain] = list(labels_in_domain)\n",
    "\n",
    "\n",
    "    idx2label = { v : k for k, v in label2idx.items()}\n",
    "    idx2word  = { v : k for k, v in word2idx.items()}\n",
    "\n",
    "    idx = {'word2idx' : word2idx, 'idx2word': idx2word, 'label2idx' : label2idx, 'idx2label' : idx2label, 'domain2label':domain2label}\n",
    "    print(\"Word : {} Label :{}\".format(len(word2idx), len(label2idx)))\n",
    "    return idx\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def build_matrix_from_domains(domains, idx, word_count, k=50) :\n",
    "\n",
    "    label_count = defaultdict(int)\n",
    "    label_word_count = np.zeros((len(idx['label2idx']), len(idx['word2idx'])))\n",
    "    for domain in domains:\n",
    "        sentences_in_domain = readCoNLL(os.path.join(DATA_DIR, domain, \"train.txt.ori\"), {0: 'tokens', 1: 'labels'})\n",
    "        for sentence_idx in range(len(sentences_in_domain)):\n",
    "            tokens = sentences_in_domain[sentence_idx]['tokens']\n",
    "            labels = sentences_in_domain[sentence_idx]['labels']\n",
    "            for token_idx in range(len(tokens)):\n",
    "                token = tokens[token_idx].lower()\n",
    "                label = get_label_name(labels[token_idx])\n",
    "                if token in idx['word2idx'].keys() and label != \"O\" and label != \"LAW\":\n",
    "                    label_count[label] += 1\n",
    "                    label_word_count[idx['label2idx'][label], idx['word2idx'][token]] += 1\n",
    "\n",
    "    original_matrix = np.zeros((len(idx['label2idx']), len(idx['word2idx'])))\n",
    "\n",
    "    for i in range(label_word_count.shape[0]):\n",
    "        for j in range(label_word_count.shape[1]):\n",
    "            if math.sqrt(label_count[idx['idx2label'][i]] * word_count[idx['idx2word'][j]]) != 0 :\n",
    "                original_matrix[i, j] = label_word_count[i,j] / math.sqrt(label_count[idx['idx2label'][i]] * word_count[idx['idx2word'][j]])\n",
    "    \n",
    "    from scipy.linalg import svd\n",
    "    M1, M2, M3 = svd(original_matrix)\n",
    "    ranked_k = M1[:, :k]\n",
    "    ranked_k_normalized = preprocessing.normalize(ranked_k, norm='l2')\n",
    "    #row_sums = M1.sum(axis=1)\n",
    "    #normalized_matrix = M1 / row_sums[:, np.newaxis]\n",
    "\n",
    "    return ranked_k_normalized\n",
    "\n",
    "def get_label_mapping(domain1, domain2, matrix, idxs) :\n",
    "    for label1 in idxs['domain2label'][domain1]:\n",
    "        highest_sim_score = -1000000000000\n",
    "        nearest_neighbor = None\n",
    "        for label2 in idxs['domain2label'][domain2]:\n",
    "            if ranked_k[idxs['label2idx'][label1]] == 0 or ranked_k[idxs['label2idx'][label2]] == 0:\n",
    "                continue\n",
    "            score = get_similarity(ranked_k[idxs['label2idx'][label1]], ranked_k[idxs['label2idx'][label2]])\n",
    "            if score > highest_sim_score:\n",
    "                highest_sim_score = score\n",
    "                nearest_neighbor = label2\n",
    "        print(\"The nearest neighbor for {} is {} with the score of {}\".format(label1, nearest_neighbor, highest_sim_score))\n",
    "\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "\n",
    "def get_similarity(repr1, repr2):\n",
    "    return 1 - cosine(repr1, repr2)\n",
    "\n",
    "def get_distance(repr1, repr2) :\n",
    "    return euclidean(repr1, repr2)\n",
    "\n",
    "def get_nearest_labels(target_task, aux_tasks, matrix, idxs, sim_threshold = 0.1):\n",
    "    nearest_labels = {}\n",
    "    \n",
    "    for aux_task in aux_tasks:\n",
    "        unique_labels = set()\n",
    "        print(\"This is the mapping between {} and {}\".format(target_task, aux_task))\n",
    "        for label1 in idxs['domain2label'][target_task]:\n",
    "            highest_sim_score = -1000000000000\n",
    "            nearest_neighbor = None\n",
    "            for label2 in idxs['domain2label'][aux_task] :\n",
    "                if not np.any(matrix[idxs['label2idx'][label1]])  or not np.any(matrix[idxs['label2idx'][label2]]):\n",
    "                    continue\n",
    "                score = get_similarity(matrix[idxs['label2idx'][label1]], matrix[idxs['label2idx'][label2]])\n",
    "                #print(\"Score between {} and {} is {}\".format(label1, label2, score))\n",
    "                if score > highest_sim_score:\n",
    "                    highest_sim_score = score\n",
    "                    nearest_neighbor = label2\n",
    "            #print(\"The nearest neighbor for {} is {} with the score of {}\".format(label1, nearest_neighbor, highest_sim_score))\n",
    "            if highest_sim_score >= sim_threshold:\n",
    "                unique_labels.add(nearest_neighbor)\n",
    "        nearest_labels[aux_task] = unique_labels\n",
    "        print(\"Nearest labels from {}  is {}\".format(aux_task, str(unique_labels)))\n",
    "    \n",
    "    return nearest_labels\n",
    "\n",
    "def compute_label_embeddings (target_task, aux_tasks):\n",
    "    \n",
    "    word_count = build_vocab(target_task + aux_tasks)\n",
    "    idxs = build_indexes_from_domains(target_task + aux_tasks, word_count)\n",
    "    matrix = build_matrix_from_domains(target_task + aux_tasks, idxs, word_count)\n",
    "    get_nearest_labels(target_task[0], aux_tasks, matrix, idxs, sim_threshold=0.1)\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of sentences in ATIS : 4478\n",
      " Number of sentences in MIT_Restaurant : 6128\n",
      " Number of sentences in MIT_Movie : 7820\n",
      " Number of sentences in CONLL_2003_NER : 14987\n",
      " Number of sentences in OntoNotes_NW : 34970\n",
      " Number of sentences in ATIS : 4478\n",
      " Number of sentences in MIT_Restaurant : 6128\n",
      " Number of sentences in MIT_Movie : 7820\n",
      " Number of sentences in CONLL_2003_NER : 14987\n",
      " Number of sentences in OntoNotes_NW : 34970\n",
      "Word : 14033 Label :119\n"
     ]
    }
   ],
   "source": [
    "word_count = build_vocab(TASKS + NERS)\n",
    "idxs = build_indexes_from_domains(TASKS + NERS, word_count)\n",
    "matrix = build_matrix_from_domains(TASKS + NERS, idxs, word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the mapping between ATIS and MIT_Restaurant\n",
      "Nearest labels from MIT_Restaurant  is {'Location', 'Rating', 'Amenity', 'Price', 'Hours'}\n",
      "This is the mapping between ATIS and MIT_Movie\n",
      "Nearest labels from MIT_Movie  is {'YEAR', 'SONG', 'PLOT', 'TITLE', 'ACTOR'}\n",
      "This is the mapping between ATIS and OntoNotes_NW\n",
      "Nearest labels from OntoNotes_NW  is {'ORDINAL', 'PRODUCT', 'EVENT', 'GPE', 'DATE', 'CARDINAL', 'WORK_OF_ART', 'FAC', 'TIME'}\n",
      "This is the mapping between ATIS and CONLL_2003_NER\n",
      "Nearest labels from CONLL_2003_NER  is {'MISC', 'ORG'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CONLL_2003_NER': {'MISC', 'ORG'},\n",
       " 'MIT_Movie': {'ACTOR', 'PLOT', 'SONG', 'TITLE', 'YEAR'},\n",
       " 'MIT_Restaurant': {'Amenity', 'Hours', 'Location', 'Price', 'Rating'},\n",
       " 'OntoNotes_NW': {'CARDINAL',\n",
       "  'DATE',\n",
       "  'EVENT',\n",
       "  'FAC',\n",
       "  'GPE',\n",
       "  'ORDINAL',\n",
       "  'PRODUCT',\n",
       "  'TIME',\n",
       "  'WORK_OF_ART'}}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nearest_labels('ATIS', ['MIT_Restaurant','MIT_Movie', 'OntoNotes_NW','CONLL_2003_NER'], matrix, idxs, sim_threshold = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukp",
   "language": "python",
   "name": "lxmls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
